{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Generator\n",
    "\n",
    "This is a character level RNN to generate new names. The data set is https://github.com/hadley/data-baby-names. I used some ideas about the general structure from an asignment in the coursera course https://www.coursera.org/learn/intro-to-deep-learning/home/info but it's mostly my own work.\n",
    "\n",
    "I use a two layer GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding, TimeDistributed, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>William</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>James</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Charles</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>George</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  sex\n",
       "0     John  boy\n",
       "1  William  boy\n",
       "2    James  boy\n",
       "3  Charles  boy\n",
       "4   George  boy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_df = pd.read_csv(\"baby-names.csv\").drop([\"year\", \"percent\"], axis = 1)\n",
    "names_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boy_names = list(names_df[\"name\"].loc[names_df[\"sex\"] == \"boy\"].str.lower().unique())\n",
    "girl_names = list(names_df[\"name\"].loc[names_df[\"sex\"] == \"girl\"].str.lower().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "tokens = list(pd.Series([char for name in map(list, [\" \"] + boy_names + girl_names) for char in name]).unique())\n",
    "num_tokens = len(tokens)\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "\n",
    "for i in range(num_tokens):\n",
    "    token_dict[tokens[i]] = i\n",
    "    \n",
    "token_dict_inv = dict([(v, k) for k, v in token_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "max_len = max(map(len, boy_names + girl_names))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_names(names_list, tokens, pad_char = \" \", width = max_len):\n",
    "    names_list = map(lambda x: pad_char + x, names_list)\n",
    "    pad = tokens[pad_char]\n",
    "    coded = []\n",
    "    for name in names_list:\n",
    "        temp = list(name)\n",
    "        temp = [tokens[char] for char in name]\n",
    "        if len(temp) > width:\n",
    "            temp = temp[:width]\n",
    "        elif len(temp) < width:\n",
    "            temp += [pad] * (width - len(temp))\n",
    "        \n",
    "        coded += [temp]\n",
    "    \n",
    "    return np.array(coded)\n",
    "\n",
    "def decode_names(code_list, tokens_inv):\n",
    "    pad = code_list[0, 0]\n",
    "    names = []\n",
    "    for code in code_list:\n",
    "        temp = \"\"\n",
    "        for char in code:\n",
    "            if char != pad:\n",
    "                temp += tokens_inv[char]\n",
    "        \n",
    "        names += [temp]\n",
    "    \n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model architecture\n",
    "def build_rnn(num_units = 128, vocab_len = num_tokens, embedding_size = 24):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(vocab_len, embedding_size))\n",
    "    model.add(GRU(num_units, return_sequences = True))\n",
    "    model.add(GRU(num_units, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(vocab_len, activation = \"softmax\")))\n",
    "    \n",
    "    model.compile(optimizer= \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# train the model to predict the next letter from the previous letters\n",
    "def train_model(model, names, vocab = token_dict, num_epochs = 1):\n",
    "    codes = encode_names(names, vocab)\n",
    "    train_X = np.delete(codes, -1, axis = 1)\n",
    "    train_y = np.delete(codes, 1, axis = 1)\n",
    "    train_y_embed = np.eye(len(vocab))[train_y]\n",
    "    \n",
    "    model.fit(train_X, train_y_embed, epochs = num_epochs, batch_size = 64)\n",
    "    \n",
    "# generate one letter from a previous sequence\n",
    "def gen_next(model, prev):\n",
    "    probs = model.predict(prev)[0, -1, :]\n",
    "    return np.random.choice(range(len(probs)), p = probs)\n",
    "\n",
    "# finally generate a sequence from a seed\n",
    "def generate_seq(model, seed = \" \", max_length = max_len, vocab = token_dict, vocab_inv = token_dict_inv):\n",
    "    name_code = [[vocab[seed]]]\n",
    "    for i in range(max_length):\n",
    "        name_code[0] += [gen_next(model, np.array(name_code))]\n",
    "    \n",
    "    return seed + decode_names(np.array(name_code), vocab_inv)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3437/3437 [==============================] - 5s - loss: 2.2838 - acc: 0.5216     \n",
      "Epoch 2/100\n",
      "3437/3437 [==============================] - 3s - loss: 1.8611 - acc: 0.5239     \n",
      "Epoch 3/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.5400 - acc: 0.5576     \n",
      "Epoch 4/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.4266 - acc: 0.5823     \n",
      "Epoch 5/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.3908 - acc: 0.5907     \n",
      "Epoch 6/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.3608 - acc: 0.5947     \n",
      "Epoch 7/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.3414 - acc: 0.5984     \n",
      "Epoch 8/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.3232 - acc: 0.6015     \n",
      "Epoch 9/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.3042 - acc: 0.6035     \n",
      "Epoch 10/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2898 - acc: 0.6056     - ETA: 0s - loss: 1.2939 - acc: 0.\n",
      "Epoch 11/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2757 - acc: 0.6079     \n",
      "Epoch 12/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2671 - acc: 0.6114     \n",
      "Epoch 13/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2537 - acc: 0.6128     \n",
      "Epoch 14/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2459 - acc: 0.6164     \n",
      "Epoch 15/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2316 - acc: 0.6192     \n",
      "Epoch 16/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2207 - acc: 0.6208     \n",
      "Epoch 17/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.2096 - acc: 0.6235     \n",
      "Epoch 18/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1972 - acc: 0.6252     \n",
      "Epoch 19/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1849 - acc: 0.6291     \n",
      "Epoch 20/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1728 - acc: 0.6312     \n",
      "Epoch 21/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1603 - acc: 0.6343     \n",
      "Epoch 22/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1496 - acc: 0.6376     \n",
      "Epoch 23/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1403 - acc: 0.6417     \n",
      "Epoch 24/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1267 - acc: 0.6431     \n",
      "Epoch 25/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1161 - acc: 0.6471     \n",
      "Epoch 26/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.1024 - acc: 0.6518     \n",
      "Epoch 27/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.0913 - acc: 0.6539     \n",
      "Epoch 28/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.0774 - acc: 0.6571     \n",
      "Epoch 29/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.0670 - acc: 0.6596     \n",
      "Epoch 30/100\n",
      "3437/3437 [==============================] - 4s - loss: 1.0532 - acc: 0.6647     \n",
      "Epoch 31/100\n",
      "3437/3437 [==============================] - 3s - loss: 1.0419 - acc: 0.6667     \n",
      "Epoch 32/100\n",
      "3437/3437 [==============================] - 3s - loss: 1.0304 - acc: 0.6691     \n",
      "Epoch 33/100\n",
      "3437/3437 [==============================] - 3s - loss: 1.0166 - acc: 0.6730     \n",
      "Epoch 34/100\n",
      "3437/3437 [==============================] - 3s - loss: 1.0042 - acc: 0.6773     \n",
      "Epoch 35/100\n",
      "3437/3437 [==============================] - 3s - loss: 0.9918 - acc: 0.6800     \n",
      "Epoch 36/100\n",
      "3437/3437 [==============================] - 3s - loss: 0.9797 - acc: 0.6833     \n",
      "Epoch 37/100\n",
      "3437/3437 [==============================] - 3s - loss: 0.9670 - acc: 0.6877     \n",
      "Epoch 38/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.9550 - acc: 0.6893     \n",
      "Epoch 39/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.9426 - acc: 0.6949     \n",
      "Epoch 40/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.9316 - acc: 0.6977     \n",
      "Epoch 41/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.9209 - acc: 0.7007     \n",
      "Epoch 42/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.9077 - acc: 0.7034     \n",
      "Epoch 43/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8957 - acc: 0.7074     \n",
      "Epoch 44/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8853 - acc: 0.7095     \n",
      "Epoch 45/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8745 - acc: 0.7126     \n",
      "Epoch 46/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8627 - acc: 0.7162     \n",
      "Epoch 47/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8523 - acc: 0.7188     \n",
      "Epoch 48/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8429 - acc: 0.7226     \n",
      "Epoch 49/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8319 - acc: 0.7251     \n",
      "Epoch 50/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8235 - acc: 0.7263     \n",
      "Epoch 51/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8130 - acc: 0.7301     \n",
      "Epoch 52/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.8031 - acc: 0.7357     \n",
      "Epoch 53/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7945 - acc: 0.7342     \n",
      "Epoch 54/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7852 - acc: 0.7381     \n",
      "Epoch 55/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7763 - acc: 0.7403     \n",
      "Epoch 56/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7696 - acc: 0.7411     \n",
      "Epoch 57/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7602 - acc: 0.7462     \n",
      "Epoch 58/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7540 - acc: 0.7457     \n",
      "Epoch 59/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7469 - acc: 0.7470     \n",
      "Epoch 60/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7392 - acc: 0.7493     \n",
      "Epoch 61/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7328 - acc: 0.7523     \n",
      "Epoch 62/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7257 - acc: 0.7529     \n",
      "Epoch 63/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7204 - acc: 0.7538     \n",
      "Epoch 64/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7141 - acc: 0.7543     \n",
      "Epoch 65/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7086 - acc: 0.7577     \n",
      "Epoch 66/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.7038 - acc: 0.7579     \n",
      "Epoch 67/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6978 - acc: 0.7596     \n",
      "Epoch 68/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6936 - acc: 0.7603     \n",
      "Epoch 69/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6884 - acc: 0.7602     \n",
      "Epoch 70/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6846 - acc: 0.7609     \n",
      "Epoch 71/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6796 - acc: 0.7619     \n",
      "Epoch 72/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6749 - acc: 0.7634     \n",
      "Epoch 73/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6709 - acc: 0.7624     \n",
      "Epoch 74/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6680 - acc: 0.7646     \n",
      "Epoch 75/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6639 - acc: 0.7659     \n",
      "Epoch 76/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6605 - acc: 0.7641     \n",
      "Epoch 77/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6572 - acc: 0.7674     \n",
      "Epoch 78/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6554 - acc: 0.7673     \n",
      "Epoch 79/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6511 - acc: 0.7689     \n",
      "Epoch 80/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6489 - acc: 0.7668     \n",
      "Epoch 81/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6458 - acc: 0.7678     \n",
      "Epoch 82/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6420 - acc: 0.7684     \n",
      "Epoch 83/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6404 - acc: 0.7683     \n",
      "Epoch 84/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6375 - acc: 0.7684     \n",
      "Epoch 85/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6354 - acc: 0.7682     \n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3437/3437 [==============================] - 4s - loss: 0.6318 - acc: 0.7701     \n",
      "Epoch 87/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6299 - acc: 0.7701     \n",
      "Epoch 88/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6279 - acc: 0.7703     \n",
      "Epoch 89/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6274 - acc: 0.7698     \n",
      "Epoch 90/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6243 - acc: 0.7716     \n",
      "Epoch 91/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6218 - acc: 0.7718     \n",
      "Epoch 92/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6212 - acc: 0.7698     \n",
      "Epoch 93/100\n",
      "3437/3437 [==============================] - 3s - loss: 0.6183 - acc: 0.7719     \n",
      "Epoch 94/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6177 - acc: 0.7703     \n",
      "Epoch 95/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6164 - acc: 0.7703     \n",
      "Epoch 96/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6141 - acc: 0.7707     \n",
      "Epoch 97/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6130 - acc: 0.7702     \n",
      "Epoch 98/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6113 - acc: 0.7720     \n",
      "Epoch 99/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6089 - acc: 0.7724     \n",
      "Epoch 100/100\n",
      "3437/3437 [==============================] - 4s - loss: 0.6075 - acc: 0.7712     \n"
     ]
    }
   ],
   "source": [
    "boys_model = build_rnn()\n",
    "train_model(boys_model, boy_names, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4018/4018 [==============================] - 6s - loss: 2.2403 - acc: 0.4912     \n",
      "Epoch 2/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.7429 - acc: 0.5156     \n",
      "Epoch 3/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.4137 - acc: 0.5662     \n",
      "Epoch 4/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.3399 - acc: 0.5879     \n",
      "Epoch 5/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.3066 - acc: 0.5923     \n",
      "Epoch 6/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.2914 - acc: 0.5952     \n",
      "Epoch 7/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.2786 - acc: 0.5968     \n",
      "Epoch 8/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.2664 - acc: 0.6005     \n",
      "Epoch 9/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.2497 - acc: 0.6027     \n",
      "Epoch 10/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.2291 - acc: 0.6071     \n",
      "Epoch 11/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.2070 - acc: 0.6128     \n",
      "Epoch 12/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.1849 - acc: 0.6171     \n",
      "Epoch 13/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.1626 - acc: 0.6233     \n",
      "Epoch 14/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.1447 - acc: 0.6291     \n",
      "Epoch 15/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.1243 - acc: 0.6318     \n",
      "Epoch 16/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.1077 - acc: 0.6387     \n",
      "Epoch 17/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.0917 - acc: 0.6432     \n",
      "Epoch 18/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.0782 - acc: 0.6482     \n",
      "Epoch 19/100\n",
      "4018/4018 [==============================] - 5s - loss: 1.0630 - acc: 0.6534     \n",
      "Epoch 20/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.0492 - acc: 0.6569     \n",
      "Epoch 21/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.0349 - acc: 0.6639     \n",
      "Epoch 22/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.0227 - acc: 0.6665     \n",
      "Epoch 23/100\n",
      "4018/4018 [==============================] - 4s - loss: 1.0092 - acc: 0.6695     \n",
      "Epoch 24/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9953 - acc: 0.6740     \n",
      "Epoch 25/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.9831 - acc: 0.6767     \n",
      "Epoch 26/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9704 - acc: 0.6806     \n",
      "Epoch 27/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9573 - acc: 0.6843     \n",
      "Epoch 28/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.9467 - acc: 0.6874     \n",
      "Epoch 29/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9325 - acc: 0.6905     \n",
      "Epoch 30/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9212 - acc: 0.6957     \n",
      "Epoch 31/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9099 - acc: 0.6960     \n",
      "Epoch 32/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.9002 - acc: 0.7009     \n",
      "Epoch 33/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8902 - acc: 0.7029     \n",
      "Epoch 34/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8784 - acc: 0.7073     \n",
      "Epoch 35/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8685 - acc: 0.7099     \n",
      "Epoch 36/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8580 - acc: 0.7128     \n",
      "Epoch 37/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8491 - acc: 0.7161     \n",
      "Epoch 38/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8387 - acc: 0.7184     \n",
      "Epoch 39/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8290 - acc: 0.7209     \n",
      "Epoch 40/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.8221 - acc: 0.7217     \n",
      "Epoch 41/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.8115 - acc: 0.7254     \n",
      "Epoch 42/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.8049 - acc: 0.7266     \n",
      "Epoch 43/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7950 - acc: 0.7304     \n",
      "Epoch 44/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7881 - acc: 0.7326     \n",
      "Epoch 45/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7813 - acc: 0.7328     \n",
      "Epoch 46/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7734 - acc: 0.7355     \n",
      "Epoch 47/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7657 - acc: 0.7397     \n",
      "Epoch 48/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7576 - acc: 0.7403     \n",
      "Epoch 49/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7532 - acc: 0.7405     \n",
      "Epoch 50/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7456 - acc: 0.7425     \n",
      "Epoch 51/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7410 - acc: 0.7435     \n",
      "Epoch 52/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7345 - acc: 0.7468     \n",
      "Epoch 53/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7284 - acc: 0.7473     \n",
      "Epoch 54/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7228 - acc: 0.7476     \n",
      "Epoch 55/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7176 - acc: 0.7489     \n",
      "Epoch 56/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.7134 - acc: 0.7498     \n",
      "Epoch 57/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7068 - acc: 0.7515     \n",
      "Epoch 58/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.7041 - acc: 0.7522     \n",
      "Epoch 59/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6983 - acc: 0.7531     \n",
      "Epoch 60/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6933 - acc: 0.7540     \n",
      "Epoch 61/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6899 - acc: 0.7555     \n",
      "Epoch 62/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6858 - acc: 0.7550     \n",
      "Epoch 63/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6824 - acc: 0.7556     - ETA: 2s - lo\n",
      "Epoch 64/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6780 - acc: 0.7568     - ET\n",
      "Epoch 65/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6749 - acc: 0.7578     \n",
      "Epoch 66/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6710 - acc: 0.7579     \n",
      "Epoch 67/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6683 - acc: 0.7590     \n",
      "Epoch 68/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6655 - acc: 0.7571     \n",
      "Epoch 69/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6624 - acc: 0.7589     \n",
      "Epoch 70/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6597 - acc: 0.7600     \n",
      "Epoch 71/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6588 - acc: 0.7598     \n",
      "Epoch 72/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6541 - acc: 0.7624     \n",
      "Epoch 73/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6529 - acc: 0.7599     \n",
      "Epoch 74/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6500 - acc: 0.7604     \n",
      "Epoch 75/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6484 - acc: 0.7607     \n",
      "Epoch 76/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6448 - acc: 0.7609     \n",
      "Epoch 77/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6434 - acc: 0.7623     \n",
      "Epoch 78/100\n",
      "4018/4018 [==============================] - 5s - loss: 0.6427 - acc: 0.7610     \n",
      "Epoch 79/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6395 - acc: 0.7626     \n",
      "Epoch 80/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6379 - acc: 0.7634     \n",
      "Epoch 81/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6360 - acc: 0.7627     \n",
      "Epoch 82/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6354 - acc: 0.7615     \n",
      "Epoch 83/100\n",
      "4018/4018 [==============================] - 3s - loss: 0.6319 - acc: 0.7641     \n",
      "Epoch 84/100\n",
      "4018/4018 [==============================] - 3s - loss: 0.6314 - acc: 0.7628     \n",
      "Epoch 85/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6299 - acc: 0.7644     \n",
      "Epoch 86/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6292 - acc: 0.7625     \n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4018/4018 [==============================] - 4s - loss: 0.6268 - acc: 0.7636     \n",
      "Epoch 88/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6254 - acc: 0.7643     \n",
      "Epoch 89/100\n",
      "4018/4018 [==============================] - 3s - loss: 0.6240 - acc: 0.7630     \n",
      "Epoch 90/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6245 - acc: 0.7629     \n",
      "Epoch 91/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6218 - acc: 0.7633     \n",
      "Epoch 92/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6197 - acc: 0.7643     \n",
      "Epoch 93/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6186 - acc: 0.7635     \n",
      "Epoch 94/100\n",
      "4018/4018 [==============================] - 3s - loss: 0.6187 - acc: 0.7634     \n",
      "Epoch 95/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6172 - acc: 0.7631     \n",
      "Epoch 96/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6161 - acc: 0.7646     \n",
      "Epoch 97/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6157 - acc: 0.7645     \n",
      "Epoch 98/100\n",
      "4018/4018 [==============================] - 3s - loss: 0.6130 - acc: 0.7649     \n",
      "Epoch 99/100\n",
      "4018/4018 [==============================] - 3s - loss: 0.6126 - acc: 0.7635     \n",
      "Epoch 100/100\n",
      "4018/4018 [==============================] - 4s - loss: 0.6128 - acc: 0.7631     \n"
     ]
    }
   ],
   "source": [
    "girls_model = build_rnn()\n",
    "train_model(girls_model, girl_names, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight hack to eliminate one or two letter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wood\n",
      "waurden\n",
      "wibelbert\n",
      "wzitney\n",
      "wilgert\n",
      "wamm\n",
      "wzitie\n",
      "wjinf\n",
      "wide\n",
      "waoron\n",
      "wayne\n",
      "wyord\n",
      "wrod\n",
      "wivery\n",
      "with\n",
      "winffore\n",
      "wiard\n",
      "wimmeverl\n",
      "wiver\n",
      "whijhery\n"
     ]
    }
   ],
   "source": [
    "# Print 20 Boys Names\n",
    "i = 0\n",
    "while i < 20:\n",
    "    name = generate_seq(boys_model, seed = \"w\").replace(\" \", \"\")\n",
    "    if len(name) > min_len:\n",
    "        print(name)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thelea\n",
      "tifra\n",
      "thdo\n",
      "tnevaela\n",
      "tnelle\n",
      "thfolauri\n",
      "throwne\n",
      "thia\n",
      "thalyda\n",
      "thewa\n",
      "thewnia\n",
      "thna\n",
      "tholaine\n",
      "trulie\n",
      "thhea\n",
      "thewaraven\n",
      "throluz\n",
      "thoerlonsia\n",
      "torelle\n",
      "thielonna\n"
     ]
    }
   ],
   "source": [
    "# Print 20 girls Names\n",
    "i = 0\n",
    "while i < 20:\n",
    "    name = generate_seq(girls_model, seed = \"t\").replace(\" \", \"\")\n",
    "    if len(name) > min_len:\n",
    "        print(name)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of those those are better than others... Still the GRU has clearly learned something. It can identify common structures in names and mostly produces things that arn't complete nonsense.\n",
    "\n",
    "I would like royalties from anyone naming their child \"Wimmeverl\" or \"Tnevaela\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
